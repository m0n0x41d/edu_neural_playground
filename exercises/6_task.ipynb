{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcc366e-85c3-41f1-85bd-95f8e385eda6",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "Let’s consider the case where the input data is color images. Convolutional neural networks were designed to process such data, so we’ll use one of the classic models for this.\n",
    "\n",
    "We will be processing the standard CIFAR10 dataset, which includes photographs of ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Each class contains 6000 images (5000 training and 1000 testing), with each image being a 32x32 pixel color image (with three RGB color channels).\n",
    "\n",
    "Due to the nature of color images, we cannot use them in their raw form immediately: first, we need to normalize the images, converting them into images with color intensity in the range of 0 to 1. This is done using the standard Normalize() function from torchvision.transforms, which typically takes standard values (mean and standard deviation) for such normalization as parameters (0.5, 0.5, 0.5).\n",
    "\n",
    "In other words, we need to perform a composition of transformations: first, convert the images to tensors, and then normalize them.\n",
    "\n",
    "The composition is performed using the standard torchvision.transforms.Compose() function. We then pass the result as the transform parameter to the CIFAR10 constructor.\n",
    "\n",
    "\n",
    "For minor technical reasons (the need for data alignment when transitioning from convolutional layers to linear layers), it is more convenient to represent it in the format of a classic model rather than just a composition of layers.\n",
    "\n",
    "The first layer, Conv2d(3, 6, 5) with the ReLU activation function, creates a set of convolutional filters. The first parameter, 3, is the number of input channels for the images (three colors). The second parameter, 6, is the number of output channels, and the third parameter is the filter size (5x5). The output is 6 filters of size 3x5x5, and the model in total has (3 * 5 * 5 + 1) * 6 = 456 parameters. The output size of the layer will be 6 * 28 * 28, where 28 = ((32 - 5) + 1).\n",
    "\n",
    "The MaxPool2d(2,2) method implements max pooling (for details on its arguments, see the link above). The kernel_size is the pooling window size, and stride is the pooling step. Thus, we reduce the output size of the layer by half: from 6 * 28 * 28 to 6 * 14 * 14.\n",
    "\n",
    "Next, the Conv2d(6, 16, 5) function is applied again, where the six output channels of the previous function are used as inputs. Now we apply 16 filters (each size 6 * 5 * 5), and the output size of the layer will be 16 * 10 * 10, where 10 = (14 - 5) + 1. The total number of parameters at this level is (5 * 5 * 6 + 1) * 16 = 2416 parameters.\n",
    "\n",
    "The next max pooling reduces this output by half, from 16 * 10 * 10 to 16 * 5 * 5.\n",
    "\n",
    "Finally, three fully connected layers (Linear) are added. Note that before these layers, we need to modify the structure of the data being passed, as convolutional layers work with two-dimensional images, while linear layers work with vector sets. This transformation is done using x.view(-1, 16 * 5 * 5).\n",
    "\n",
    "The first linear layer with 120 nodes receives 16 * 5 * 5 inputs, requiring (16 * 5 * 5 + 1) * 120 = 48120 parameters, and then the number of inputs and outputs is reduced through the following layers to our final 10 classes (the last level requires (84+1) * 10 = 850 parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d5b0ea-5fad-4f02-8a39-12f4fcba7208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "10000\n",
      "0.9823124408721924\n",
      "Accuracy: 56.02 %\n",
      "Accuracy for tensor(3) : 63 %\n",
      "Accuracy for tensor(5) : 69 %\n",
      "Accuracy for tensor(1) : 53 %\n",
      "Accuracy for tensor(7) : 26 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import Linear, Sigmoid\n",
    "\n",
    "input_size = 3*32*32   # Image size in pixels * number of colors\n",
    "num_classes = 10       # Number of recognized classes (10 types of images)\n",
    "n_epochs = 2           # Number of epochs\n",
    "batch_size = 4         # Mini-batch size of input data\n",
    "lr = 0.001             # Learning rate\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Download Training and testing CIFAR10 datasets\n",
    "cifar_trainset = dsets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar_testset = dsets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "print(len(cifar_trainset))\n",
    "print(len(cifar_testset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=cifar_trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=cifar_testset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# our beloved train_step func :) \n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CifarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CifarModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "from torch import optim, nn\n",
    "\n",
    "model = CifarModel()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss = train_step(images, labels)\n",
    "\n",
    "# print(model.state_dict())\n",
    "print(loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy: {} %'.format(100 * correct / total))\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy for %5s : %2d %%' % (\n",
    "        labels[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91355ba8-5848-4fee-9d6d-974ab9beaea6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Great! We should get something around 55%!\n",
    "\n",
    "And  this is only for 2 epochs! The good news that simply increasing the number of epochs we can improve accuracy.\n",
    "After 10 epochs our model will be 80% accurate.\n",
    "\n",
    "But it is not cool, because it is too simple, right? Lets challenge ourself and try to find *another ways* to increase model accuraccy (No increasing enochs).\n",
    "\n",
    "---\n",
    "\n",
    "We are going to add *two additional convolutional layers*, and also increase the number of filters to 32, 64, 128, and 256, which should allow the model to capture more complex patterns. Besides that, we will add a Dropout layer after the first fully connected one to reduce the possibility of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04dd7ac-a012-4e40-b9ee-6435157f490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "10000\n",
      "Epoch [1/2], Loss: 1.1228\n",
      "Epoch [2/2], Loss: 1.0673\n",
      "Accuracy: 63.77 %\n",
      "Accuracy for tensor(7) : 87 %\n",
      "Accuracy for tensor(5) : 76 %\n",
      "Accuracy for tensor(8) : 48 %\n",
      "Accuracy for tensor(0) : 31 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import Linear, Sigmoid\n",
    "\n",
    "input_size = 3*32*32   # Image size in pixels * number of colors\n",
    "num_classes = 10       # Number of recognized classes (10 types of images)\n",
    "n_epochs = 2           # Number of epochs\n",
    "batch_size = 64        # Increased batch size\n",
    "lr = 0.001             # Learning rate\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Download Training and testing CIFAR10 datasets\n",
    "cifar_trainset = dsets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar_testset = dsets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "print(len(cifar_trainset))\n",
    "print(len(cifar_testset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=cifar_trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=cifar_testset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# our beloved train_step func :) \n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step\n",
    "\n",
    "class ImprovedCifarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCifarModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ImprovedCifarModel()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss = train_step(images, labels)\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "with torch.no_grad(): # checking on testing dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy: {} %'.format(100 * correct / total))\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy for %5s : %2d %%' % (\n",
    "        labels[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406fe39-65eb-46af-94ec-f66de962cc24",
   "metadata": {},
   "source": [
    "We should achieve a total accuracy of around 61-66% now! Wow! I'm not sure what the spread is due to, but we can still increase the number of epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
